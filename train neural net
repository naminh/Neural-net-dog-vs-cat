function model = train_nnV2(conf,Ws,bs,trn_dat,trn_lab,vld_dat,vld_lab,metrics)

[sz,visNum] = size(trn_dat);
depth = length(conf.hidNum)+1;
labNum = size(unique(trn_lab),1);
vld_acc = 0; % for conf.ESTOP to work, to compare overfitting when train accuracy diverges from val acc
acc_drop_count = 0; % for conf.E_STOP_LR_REDUCE to work
valAcc = 0; % count bad epochs
vld_best = 0; % keep score of best validation accuracy

if isempty(Ws) % if no initial weights given
    model.Ws{1} = (1/visNum)*(2*rand(visNum,conf.hidNum(1))-1); % initialize weights from input to hidden neurons
    DW{1} = zeros(size(model.Ws{1})); % for weight adjustments
    for i=2:depth-1
        model.Ws{i} = (1/conf.hidNum(i-1))*(2*rand(conf.hidNum(i-1),conf.hidNum(i))-1); % weights between hidden layers
        DW{i} = zeros(size(model.Ws{i}));
    end
    model.Ws{depth} = (1/conf.hidNum(depth-1))*(2*rand(conf.hidNum(depth-1),labNum)-1); % weights from hidden to output layer
    DW{depth} = zeros(size(model.Ws{depth}));
end

if isempty(bs)
 % Initialize biases
    for i=1:depth-1
        model.bs{i} = zeros(1,conf.hidNum(i));
        DB{i} = model.bs{i};
    end
    model.bs{depth} = zeros(1,labNum);
    DB{depth} = model.bs{depth};
end

plot_trn_acc = [];
plot_vld_acc = [];
plot_mse = [];
    for e=1:conf.eNum % epochs
        MSE = 0;
        for b=1:conf.bNum % batch updates
           inx = (b-1)*conf.sNum+1:min(b*conf.sNum,sz);
           batch_x = trn_dat(inx,:);
           batch_y = trn_lab(inx)+1;
           sNum = size(batch_x,1);
           % Forward mesage to get output
           input{1} = bsxfun(@plus,batch_x*model.Ws{1},model.bs{1}); % calculate (input * weights + bias)
           actFunc=  str2func(conf.activationFnc{1}); % apply activation function
           output{1} = actFunc(input{1});
           for i=2:depth
               input{i} = bsxfun(@plus,output{i-1}*model.Ws{i},model.bs{i});
               actFunc=  str2func(conf.activationFnc{i});
               output{i} = actFunc(input{i});
           end  
           % Back-prop update        
           y = discrete2softmax(batch_y,labNum); % vectorise labels
           err{depth} = (y-output{depth}).*deriv(conf.activationFnc{depth},input{depth});       
           [~,cout] = max(output{depth},[],2); % extract labels

           MSE = MSE + mean(sqrt(mean((output{depth}-y).^2)));
           for i=depth:-1:2
               diff = output{i-1}'*err{i}/sNum;
               DW{i} = conf.params(1)*(diff - conf.params(4)*model.Ws{i}) + conf.params(3)*DW{i};
               model.Ws{i} = model.Ws{i} + DW{i}; % adjust weights from top to bottom

               DB{i} = conf.params(1)*mean(err{i}) + conf.params(3)*DB{i};
               model.bs{i} = model.bs{i} + DB{i}; % adjust biases
               err{i-1} = err{i}*model.Ws{i}'.*deriv(conf.activationFnc{i},input{i-1});
           end
           diff = batch_x'*err{1}/sNum;        
           DW{1} = conf.params(1)*(diff - conf.params(4)*model.Ws{1}) + conf.params(3)*DW{1};
           model.Ws{1} = model.Ws{1} + DW{1}; % adjust weights in first layer      

           DB{1} = conf.params(1)*mean(err{1}) + conf.params(3)*DB{1};
           model.bs{1} = model.bs{1} + DB{1}; % adjust biases in first layer      
       end

       % Get training classification error
       cout = run_nn(conf.activationFnc,model,trn_dat); % predict label using the model
       trn_acc = sum((cout-1)==trn_lab)/size(trn_lab,1); % calculate accuracy for training set
       cout = run_nn(conf.activationFnc,model,vld_dat);
       
       % Testing if validation accuracy increases compared to last epoch
       if vld_acc >= sum((cout-1)==vld_lab)/size(vld_lab,1) % compare val accuracy with the one from previous epoch
           valAcc = valAcc + 1;
       else
           valAcc = 0;
       end

       vld_acc = sum((cout-1)==vld_lab)/size(vld_lab,1); % set validation accuracy

       % If metrics equal to 1 then print the accuracies per epoch
       if metrics
           fprintf('[Eppoch %4d] MSE = %.5f| Train acc = %.5f|Validation = %.5f\n',e,MSE,trn_acc,vld_acc);       
           % Collect data for plot
           plot_trn_acc = [plot_trn_acc trn_acc];
           plot_vld_acc = [plot_vld_acc vld_acc];
           plot_mse     = [plot_mse MSE];
       end
       
       % Stop training when validation accuracy does not increase after conf.E_STOP epochs
       if isfield(conf,'E_STOP')
           if valAcc >= conf.E_STOP
               break
           end
       end
       
       % Learning rate decay if model does not improve. We set back to the
       % best model and continue there with a lower learning rate
       if isfield(conf,'E_STOP_LR_REDUCE')
           if vld_acc<vld_best
               acc_drop_count = acc_drop_count + 1;
               if acc_drop_count >= conf.E_STOP_LR_REDUCE
                   fprintf('Learning rate reduced!\n');
                   acc_drop_count = 0;
                   es_count = es_count + 1; % number of reduce learning rate
                   conf.params(1) = conf.params(1)/10; % reduce learning rate after acc_drop_count number of bad epochs
                   model = model_best;
               end
           else
               es_count = 0;
               acc_drop_count = 0;
               vld_best = vld_acc;
               model_best = model;
           end
       end
       
       % Stop training when training exceeds validation accuracy by conf.thresh           
       if isfield(conf,'thresh')
           if abs(vld_acc-trn_acc) > conf.thresh
               break 
           end
       end
    end
    
model = model_best;
    
% If metrics equal to 1 then plot accuracies
if metrics
    fig1 = figure(1);
    set(fig1,'Position',[10,20,300,200]);
    plot(1:size(plot_trn_acc,2),plot_trn_acc,'r');
    hold on;
    plot(1:size(plot_vld_acc,2),plot_vld_acc);    
    legend('Training','Validation');
    xlabel('Epochs');ylabel('Accuracy');

    fig2 = figure(2);
    set(fig2,'Position',[10,20,300,200]);
    plot(1:size(plot_mse,2),plot_mse);    
    xlabel('Epochs');ylabel('MSE');
end

end
